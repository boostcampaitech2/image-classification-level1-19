{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geographic-foster",
   "metadata": {
    "id": "geographic-foster",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Lesson 4 - Data Generation\n",
    "- 이번 실습자료에서는 파이토치 모델에 이미지를 입력값으로 주기위해 전처리를 하는 방법을 배웁니다.\n",
    "- 파이토치는 torch.utils.data에 있는 Dataset, DataLoader 클래스가 이 작업을 간편하게 해줍니다.\n",
    "## 0. Libraries & Configurations\n",
    "1- 시각화에 필요한 라이브러리와 데이터 경로를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "occasional-boxing",
   "metadata": {
    "id": "occasional-boxing"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import f1_score\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import resnet18, resnet50, resnet101\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "#define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complex-israel",
   "metadata": {
    "id": "complex-israel"
   },
   "outputs": [],
   "source": [
    "### Configurations\n",
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/train_with_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "whole-computer",
   "metadata": {
    "id": "whole-computer"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_path)\n",
    "df_idx = {\"id\":0, \"gender\":1, \"age\":2, \"path\":3, \"name\":4, \"label\":5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-neighborhood",
   "metadata": {
    "id": "minute-neighborhood"
   },
   "source": [
    "## 1. Image Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crude-frank",
   "metadata": {
    "id": "crude-frank"
   },
   "outputs": [],
   "source": [
    "def get_ext(img_dir, img_id):\n",
    "    filename = os.listdir(os.path.join(img_dir, img_id))[0]\n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "superb-profession",
   "metadata": {
    "id": "superb-profession"
   },
   "outputs": [],
   "source": [
    "def get_img_stats(img_ids):\n",
    "    img_info = dict(heights=[], widths=[], means=[], stds=[])\n",
    "    for path in tqdm.tqdm(img_ids):\n",
    "        img = np.array(Image.open(path))\n",
    "        h, w, _ = img.shape\n",
    "        img_info['heights'].append(h)\n",
    "        img_info['widths'].append(w)\n",
    "        img_info['means'].append(img.mean(axis=(0,1)))\n",
    "        img_info['stds'].append(img.std(axis=(0,1)))\n",
    "    return img_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9630245d-1c19-4a03-9490-fc9830f511fa",
   "metadata": {
    "id": "undefined-patrol",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18900/18900 [06:50<00:00, 46.04it/s]\n"
     ]
    }
   ],
   "source": [
    "img_info = get_img_stats(df.path.values)\n",
    "#print(f'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}')\n",
    "#print(f'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da16d75-7b84-4209-812c-795ddaf67a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB Mean: [0.56019358 0.52410121 0.501457  ]\n",
      "RGB Standard Deviation: [0.23318603 0.24300033 0.24567522]\n"
     ]
    }
   ],
   "source": [
    "print(f'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}')\n",
    "print(f'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-juvenile",
   "metadata": {
    "id": "democratic-juvenile",
    "tags": []
   },
   "source": [
    "## 2.1 Augmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "horizontal-strain",
   "metadata": {
    "id": "horizontal-strain"
   },
   "outputs": [],
   "source": [
    "mean, std = (0.5, 0.5, 0.5), (0.2, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "national-diameter",
   "metadata": {
    "id": "national-diameter"
   },
   "outputs": [],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "divine-transmission",
   "metadata": {
    "id": "divine-transmission"
   },
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    class_labels = []\n",
    "    image_images = []\n",
    "    \n",
    "    def __init__(self, data_df, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "        self.df = data_df\n",
    "\n",
    "        self.setup()\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        for index in tqdm.tqdm(range(self.__len__())):\n",
    "            df_series = self.df.iloc[index]\n",
    "            img_path = df_series[df_idx[\"path\"]]\n",
    "            if os.path.exists(img_path):\n",
    "                self.image_images.append(self.transform(image=np.array(Image.open(img_path)))['image'])\n",
    "                self.class_labels.append(df_series[df_idx[\"label\"]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        #image_path = self.image_paths[index]\n",
    "        image = self.image_images[index]\n",
    "        class_label = self.class_labels[index]\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        #image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image, class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "582e99c2-969b-4d3c-952b-e819be35db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7eaff3d-e76b-4b45-aab2-40225263ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "69a03de2-be3b-45f2-9ab2-022ac06a0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = []\n",
    "for drop_idx in train_df.index:\n",
    "    if train_df.loc[drop_idx]['label'] == 0:\n",
    "        if np.random.randint(6) != 0:\n",
    "            drop_list.append(drop_idx)\n",
    "    elif train_df.loc[drop_idx]['label'] == 1:\n",
    "        if np.random.randint(5) != 0:\n",
    "            drop_list.append(drop_idx)\n",
    "    elif train_df.loc[drop_idx]['label'] == 3:\n",
    "        if np.random.randint(9) != 0:\n",
    "            drop_list.append(drop_idx)\n",
    "    elif train_df.loc[drop_idx]['label'] == 4:\n",
    "        if np.random.randint(10) != 0:\n",
    "            drop_list.append(drop_idx)\n",
    "    elif train_df.loc[drop_idx]['label'] in [9, 10, 12, 15, 16]:\n",
    "        if np.random.randint(2) != 0:\n",
    "            drop_list.append(drop_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "503c5f98-5dcd-47cb-91f3-d4a44a23f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train_df = train_df.drop(drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2d2f851d-0c8e-4e1f-90c9-850b88ea4cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0     355\n",
       "1     330\n",
       "2     324\n",
       "3     349\n",
       "4     296\n",
       "5     437\n",
       "6     443\n",
       "7     316\n",
       "8      64\n",
       "9     302\n",
       "10    316\n",
       "11     87\n",
       "12    226\n",
       "13    319\n",
       "14     67\n",
       "15    301\n",
       "16    329\n",
       "17     91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_train_df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "secure-plasma",
   "metadata": {
    "id": "secure-plasma"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4952/4952 [02:06<00:00, 39.29it/s]\n",
      "100%|██████████| 3780/3780 [00:28<00:00, 134.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.18702936172485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "start = time()\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "train_dataset = MaskBaseDataset(\n",
    "    data_df=drop_train_df, transform=transform[\"train\"]\n",
    ")\n",
    "\n",
    "val_dataset = MaskBaseDataset(\n",
    "    data_df=val_df, transform=transform[\"val\"]\n",
    ")\n",
    "\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1462c2e4-a685-4d6c-b076-a2621e01c648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3780"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-membrane",
   "metadata": {
    "id": "regulation-membrane"
   },
   "source": [
    "## 3. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "blessed-robert",
   "metadata": {
    "id": "blessed-robert"
   },
   "outputs": [],
   "source": [
    "# training dataloader은 데이터를 섞어주어야 합니다. (shuffle=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5c4f390a-f6c2-4bd4-a93c-4258080252a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cd0e901e-1fb7-4b3a-94a8-bc7a9c75b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b350aa32-c6db-44ea-9a91-15a575ab331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_finetune(classes):\n",
    "    #model = EfficientNet.from_pretrained('efficientnet-b2', num_classes=classes)\n",
    "    model = resnet18(pretrained=True)\n",
    "    # class 18개로 분리\n",
    "    model.fc = torch.nn.Linear(in_features=512, out_features=classes, bias=True)\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "    stdv = 1/math.sqrt(512)\n",
    "    model.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "37a8c9f7-eab2-42c5-8318-96fb111ff383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c3be6aee-9549-4e6c-a18c-61861e982daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_resnet = resnet_finetune(18).to(device)\n",
    "loss_fn = (torch.nn.CrossEntropyLoss())\n",
    "optimizer = torch.optim.Adam(my_resnet.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eeb0211b-2e8f-4825-ade8-04ba5d485a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_loader, \"test\": val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ff09e0fc-cf5f-4a29-90bf-b8ecd43e6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "f1_best = 0\n",
    "count = 0\n",
    "epoch_f1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "287b2a6c-421c-4591-b8be-5055e1634035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 1.823, 평균 Accuracy : 0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.744, 평균 Accuracy : 0.777\n",
      "best :  0.6306650950905457 , last :  0.6306650950905457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.580, 평균 Accuracy : 0.840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss : 0.409, 평균 Accuracy : 0.881\n",
      "best :  0.8296442177523793 , last :  0.8296442177523793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.318, 평균 Accuracy : 0.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss : 0.227, 평균 Accuracy : 0.950\n",
      "best :  0.9266493117779072 , last :  0.9266493117779072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss : 0.183, 평균 Accuracy : 0.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 test-데이터 셋에서 평균 Loss : 0.128, 평균 Accuracy : 0.987\n",
      "best :  0.9782141808550794 , last :  0.9782141808550794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss : 0.105, 평균 Accuracy : 0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-4의 test-데이터 셋에서 평균 Loss : 0.074, 평균 Accuracy : 0.998\n",
      "best :  0.9972550176967858 , last :  0.9972550176967858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-5의 train-데이터 셋에서 평균 Loss : 0.061, 평균 Accuracy : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-5의 test-데이터 셋에서 평균 Loss : 0.041, 평균 Accuracy : 1.000\n",
      "best :  0.9997286663953331 , last :  0.9997286663953331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-6의 train-데이터 셋에서 평균 Loss : 0.037, 평균 Accuracy : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-6의 test-데이터 셋에서 평균 Loss : 0.029, 평균 Accuracy : 1.000\n",
      "best :  1.0 , last :  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-7의 train-데이터 셋에서 평균 Loss : 0.025, 평균 Accuracy : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-7의 test-데이터 셋에서 평균 Loss : 0.020, 평균 Accuracy : 1.000\n",
      "best :  1.0 , last :  1.0\n",
      "early stop --------------------\n",
      "학습 종료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        if phase == \"train\":\n",
    "            my_resnet.train()\n",
    "        elif phase == \"test\":\n",
    "            my_resnet.eval()\n",
    "        \n",
    "        # 배치 단위로 data load하여서 작업 -> 이때 transpose 및 여러 함수가 적용된다.\n",
    "        epoch_f1 = 0\n",
    "        n_iters=0\n",
    "        for ind, (images, labels) in enumerate(tqdm.tqdm(dataloaders[phase], leave=False)):\n",
    "            images = torch.stack(list(images), dim=0).to(device)\n",
    "            labels = torch.tensor(list(labels)).to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # parameter gradient를 업데이트 전 초기화함\n",
    "\n",
    "            with torch.set_grad_enabled(\n",
    "                phase == \"train\"\n",
    "            ):  \n",
    "                logits = my_resnet(images)\n",
    "                _, preds = torch.max(\n",
    "                    logits, 1\n",
    "                )\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()  # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient\n",
    "                    optimizer.step()  # 계산된 gradient를 가지고 모델 업데이트\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_acc += torch.sum(\n",
    "                preds == labels.data\n",
    "            )\n",
    "            epoch_f1 += f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "            n_iters += 1\n",
    "\n",
    "        # 한 epoch이 모두 종료되었을 때,\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}\")\n",
    "        if phase == \"test\":\n",
    "            epoch_f1 /= n_iters\n",
    "            if epoch_f1 > f1_best:\n",
    "                f1_best = epoch_f1\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "            print(\"best : \", f1_best, \", last : \", epoch_f1)\n",
    "    if count == 1:\n",
    "        print(\"early stop\", \"-\"*20)\n",
    "        break\n",
    "print(\"학습 종료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d4f75d3-d4b4-4343-a37e-7043c0d19658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best :  0.9438333793859479 , last :  0.8850687592629923\n"
     ]
    }
   ],
   "source": [
    "print(\"best : \", f1_best, \", last : \", epoch_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "48ca732b-c694-4788-b1bb-556dc04e0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = '/opt/ml/input/data/eval'\n",
    "test_dir = \"/opt/ml/input/data/eval\"\n",
    "submission = pd.read_csv(os.path.join(test_dir, \"info.csv\"))\n",
    "image_dir = os.path.join(test_dir, \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "52a08649-9b79-46b1-947a-ed89d6856cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d83c7e9-42de-41af-8409-e6a38ffec1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "99fdc55e-0ecf-4814-9b73-b2030b250d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = torchvision.transforms.Compose([ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "251b0ef2-b515-4e1f-9e56-a315cbbb4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TestDataset(image_paths, test_transform)\n",
    "loader = data.DataLoader(dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fe2f1b5f-c105-42b7-947d-afc7847c9ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>d71d4570505d6af8f777690e63edfa8d85ea4476.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>6cf1300e8e218716728d5820c0bab553306c2cfd.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>8140edbba31c3a824e817e6d5fb95343199e2387.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ImageID  ans\n",
       "0      cbc5c6e168e63498590db46022617123f1fe1268.jpg   13\n",
       "1      0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    6\n",
       "2      b549040c49190cedc41327748aeb197c1670f14d.jpg   13\n",
       "3      4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg   13\n",
       "4      248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg   12\n",
       "...                                             ...  ...\n",
       "12595  d71d4570505d6af8f777690e63edfa8d85ea4476.jpg    0\n",
       "12596  6cf1300e8e218716728d5820c0bab553306c2cfd.jpg    5\n",
       "12597  8140edbba31c3a824e817e6d5fb95343199e2387.jpg    6\n",
       "12598  030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg    2\n",
       "12599  f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg    8\n",
       "\n",
       "[12600 rows x 2 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions = []\n",
    "my_resnet.eval()\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = my_resnet(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "print(\"test inference is done!\")\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bd60a8b7-5ad8-4f4f-b591-1043eab98213",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('/opt/ml/input/data/eval/submission_resnet18_10_0001_256_undersample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653bc78-8a1d-4b6c-b4cd-597a3cb6876e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_DataGeneration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
