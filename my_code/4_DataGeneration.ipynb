{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geographic-foster",
   "metadata": {
    "id": "geographic-foster"
   },
   "source": [
    "# Lesson 4 - Data Generation\n",
    "- 이번 실습자료에서는 파이토치 모델에 이미지를 입력값으로 주기위해 전처리를 하는 방법을 배웁니다.\n",
    "- 파이토치는 torch.utils.data에 있는 Dataset, DataLoader 클래스가 이 작업을 간편하게 해줍니다.\n",
    "## 0. Libraries & Configurations\n",
    "- 시각화에 필요한 라이브러리와 데이터 경로를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "occasional-boxing",
   "metadata": {
    "id": "occasional-boxing"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import f1_score\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import resnet18, resnet50, resnet101\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complex-israel",
   "metadata": {
    "id": "complex-israel"
   },
   "outputs": [],
   "source": [
    "### Configurations\n",
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "whole-computer",
   "metadata": {
    "id": "whole-computer"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  gender   race  age                    path\n",
       "0  000001  female  Asian   45  000001_female_Asian_45\n",
       "1  000002  female  Asian   52  000002_female_Asian_52\n",
       "2  000004    male  Asian   54    000004_male_Asian_54\n",
       "3  000005  female  Asian   58  000005_female_Asian_58\n",
       "4  000006  female  Asian   59  000006_female_Asian_59"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(df_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-neighborhood",
   "metadata": {
    "id": "minute-neighborhood"
   },
   "source": [
    "## 1. Image Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-protest",
   "metadata": {
    "id": "christian-protest"
   },
   "source": [
    "- 2강 실습자료에서 사용되었던 데이터셋의 RGB 평균, 표준편차를 구하는 함수를 이용하여 그에 대해 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "crude-frank",
   "metadata": {
    "id": "crude-frank"
   },
   "outputs": [],
   "source": [
    "def get_ext(img_dir, img_id):\n",
    "    filename = os.listdir(os.path.join(img_dir, img_id))[0]\n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "superb-profession",
   "metadata": {
    "id": "superb-profession"
   },
   "outputs": [],
   "source": [
    "def get_img_stats(img_dir, img_ids):\n",
    "    img_info = dict(heights=[], widths=[], means=[], stds=[])\n",
    "    for img_id in tqdm(img_ids):\n",
    "        for path in glob(os.path.join(img_dir, img_id, '*')):\n",
    "            img = np.array(Image.open(path))\n",
    "            h, w, _ = img.shape\n",
    "            img_info['heights'].append(h)\n",
    "            img_info['widths'].append(w)\n",
    "            img_info['means'].append(img.mean(axis=(0,1)))\n",
    "            img_info['stds'].append(img.std(axis=(0,1)))\n",
    "    return img_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "undefined-patrol",
   "metadata": {
    "id": "undefined-patrol",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2700/2700 [06:42<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB Mean: [0.56019358 0.52410121 0.501457  ]\n",
      "RGB Standard Deviation: [0.23318603 0.24300033 0.24567522]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_info = get_img_stats(img_dir, df.path.values)\n",
    "\n",
    "print(f'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}')\n",
    "print(f'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-operation",
   "metadata": {
    "id": "compliant-operation"
   },
   "source": [
    "## 2. Dataset\n",
    "- 이 부분에서는 Dataset을 정의하는 방법을 간단하게 배웁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-juvenile",
   "metadata": {
    "id": "democratic-juvenile"
   },
   "source": [
    "## 2.1 Augmentation Function\n",
    "- 3강에서 배운 Augmentation 함수를 정의합니다.\n",
    "- mean, std는 임의로 설정하였으나 파트 1에서 계산한 값을 입력해도 괜찮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "horizontal-strain",
   "metadata": {
    "id": "horizontal-strain"
   },
   "outputs": [],
   "source": [
    "mean, std = (0.5, 0.5, 0.5), (0.2, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-recycling",
   "metadata": {
    "id": "distant-recycling"
   },
   "source": [
    "### 2.1.1 Torchvision Style Augmentation Function\n",
    "- Torchvision에서 제공되는 transforms를 이용한 Augmentation 함수입니다.\n",
    "- 이를 사용하여 Dataset을 정의하여도 괜찮지만, 이번 실습자료에서는 강의에서 배웠던 Albumentation을 활용해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-hometown",
   "metadata": {
    "id": "current-hometown",
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Torchvision-Style Transforms\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, GaussianBlur, RandomRotation, ColorJitter\n",
    "\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384)):\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = transforms.Compose([\n",
    "            Resize((img_size[0], img_size[1])),\n",
    "            RandomRotation([-8, +8]),\n",
    "            GaussianBlur(51, (0.1, 2.0)),\n",
    "            ColorJitter(brightness=0.5, saturation=0.5, hue=0.5),  # todo : param\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "            AddGaussianNoise(0., 1.)\n",
    "        ])\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = transforms.Compose([\n",
    "            Resize((img_size[0], img_size[1])),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    return transformations\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-survivor",
   "metadata": {
    "id": "sudden-survivor"
   },
   "source": [
    "### 2.1.2 Albumentation Style Augmentation Function\n",
    "- Albumentation은 numpy 형식으로 이미지를 받아 데이터를 변형시킵니다.\n",
    "- opencv 기반으로 빠르고, 다양한 Augmentation 방법이 제공되는 점에서 장점이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "national-diameter",
   "metadata": {
    "id": "national-diameter"
   },
   "outputs": [],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-lafayette",
   "metadata": {
    "id": "partial-lafayette"
   },
   "source": [
    "### 2.2 Define Dataset\n",
    "\n",
    "- 여기에서는 이미지와 레이블을 출력하는 Dataset 클래스를 정의합니다.\n",
    "- 레이블은 마스크 여부, 성별, 나이로 결정이 됩니다.\n",
    "- 레이블은 3(마스크 착용, 미착용, 잘못착용) * 2(남성, 여성) * 3(30세 미만, 30세-60세, 60세 이상) 으로 총 18개가 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "annoying-emission",
   "metadata": {
    "id": "annoying-emission"
   },
   "outputs": [],
   "source": [
    "### 마스크 여부, 성별, 나이를 mapping할 클래스를 생성합니다.\n",
    "\n",
    "class MaskLabels:\n",
    "    mask = 0\n",
    "    incorrect = 1\n",
    "    normal = 2\n",
    "\n",
    "class GenderLabels:\n",
    "    male = 0\n",
    "    female = 1\n",
    "\n",
    "class AgeGroup:\n",
    "    map_label = lambda x: 0 if int(x) < 30 else 1 if int(x) < 60 else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "divine-transmission",
   "metadata": {
    "id": "divine-transmission"
   },
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1.jpg\": MaskLabels.mask,\n",
    "        \"mask2.jpg\": MaskLabels.mask,\n",
    "        \"mask3.jpg\": MaskLabels.mask,\n",
    "        \"mask4.jpg\": MaskLabels.mask,\n",
    "        \"mask5.jpg\": MaskLabels.mask,\n",
    "        \"incorrect_mask.jpg\": MaskLabels.incorrect,\n",
    "        \"normal.jpg\": MaskLabels.normal\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            for file_name, label in self._file_names.items():\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                if os.path.exists(img_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_labels.append(label)\n",
    "\n",
    "                    id, gender, race, age = profile.split(\"_\")\n",
    "                    gender_label = getattr(GenderLabels, gender)\n",
    "                    age_label = AgeGroup.map_label(age)\n",
    "\n",
    "                    self.gender_labels.append(gender_label)\n",
    "                    self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image_transform, multi_class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "secure-plasma",
   "metadata": {
    "id": "secure-plasma"
   },
   "outputs": [],
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "dataset = MaskBaseDataset(\n",
    "    img_dir=img_dir\n",
    ")\n",
    "\n",
    "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_train = len(dataset) - n_val\n",
    "train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# 각 dataset에 augmentation 함수를 설정합니다.\n",
    "train_dataset.dataset.set_transform(transform['train'])\n",
    "val_dataset.dataset.set_transform(transform['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5311e20-b69f-4e18-bf24-3717ba0b8904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18035"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "regulation-membrane",
   "metadata": {
    "id": "regulation-membrane"
   },
   "source": [
    "## 3. DataLoader\n",
    "- 정의한 Dataset을 바탕으로 DataLoader을 생성합니다.\n",
    "- Dataset은 이미지 한장을 주는 모듈이라면, DataLoader은 여러 이미지를 batch_size만큼 묶어 전달해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "blessed-robert",
   "metadata": {
    "id": "blessed-robert"
   },
   "outputs": [],
   "source": [
    "# training dataloader은 데이터를 섞어주어야 합니다. (shuffle=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "361fd857-a344-4a5c-801a-a4b9174de257",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de1ab561-a2d6-4e83-b1f3-44ccf8c37980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_finetune(classes):\n",
    "    #model = EfficientNet.from_pretrained('efficientnet-b2', num_classes=classes)\n",
    "    model = resnet18(pretrained=True)\n",
    "    # class 18개로 분리\n",
    "    model.fc = torch.nn.Linear(in_features=512, out_features=classes, bias=True)\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "    stdv = 1/math.sqrt(512)\n",
    "    model.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77c2c292-a087-46db-a446-3e6292b5c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_resnet = resnet_finetune(18).to(device)\n",
    "loss_fn = (torch.nn.CrossEntropyLoss())\n",
    "optimizer = torch.optim.Adam(my_resnet.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae807d51-05c6-4107-bc4d-084adabfe6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_loader, \"test\": val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e85e8e6-7f5f-4f60-92e1-ba50f6ce10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_best = 0\n",
    "count = 0\n",
    "epoch_f1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec4f990d-3e74-4b2c-be38-70c8ce8d197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c612ef9d-dcb4-4f9a-9a11-ed7c303417c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.613, 평균 Accuracy : 0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/113 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.229, 평균 Accuracy : 0.923\n",
      "best :  0.8353639948034196 , last :  0.8353639948034196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.113, 평균 Accuracy : 0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/113 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss : 0.117, 평균 Accuracy : 0.960\n",
      "best :  0.9019491563960387 , last :  0.9019491563960387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.029, 평균 Accuracy : 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/113 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss : 0.106, 평균 Accuracy : 0.969\n",
      "best :  0.9398580203903506 , last :  0.9398580203903506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss : 0.012, 평균 Accuracy : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/113 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 test-데이터 셋에서 평균 Loss : 0.064, 평균 Accuracy : 0.980\n",
      "best :  0.9571872428594134 , last :  0.9571872428594134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-90d4b7be7534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        if phase == \"train\":\n",
    "            my_resnet.train()\n",
    "        elif phase == \"test\":\n",
    "            my_resnet.eval()\n",
    "        \n",
    "        # 배치 단위로 data load하여서 작업 -> 이때 transpose 및 여러 함수가 적용된다.\n",
    "        epoch_f1 = 0\n",
    "        n_iters=0\n",
    "        for ind, (images, labels) in enumerate(tqdm(dataloaders[phase], leave=False)):\n",
    "            images = torch.stack(list(images), dim=0).to(device)\n",
    "            labels = torch.tensor(list(labels)).to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # parameter gradient를 업데이트 전 초기화함\n",
    "\n",
    "            with torch.set_grad_enabled(\n",
    "                phase == \"train\"\n",
    "            ):  \n",
    "                logits = my_resnet(images)\n",
    "                _, preds = torch.max(\n",
    "                    logits, 1\n",
    "                )\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()  # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient\n",
    "                    optimizer.step()  # 계산된 gradient를 가지고 모델 업데이트\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_acc += torch.sum(\n",
    "                preds == labels.data\n",
    "            )\n",
    "            epoch_f1 += f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "            n_iters += 1\n",
    "\n",
    "        # 한 epoch이 모두 종료되었을 때,\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}\")\n",
    "        if phase == \"test\":\n",
    "            epoch_f1 /= n_iters\n",
    "            if epoch_f1 > f1_best:\n",
    "                f1_best = epoch_f1\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "            print(\"best : \", f1_best, \", last : \", epoch_f1)\n",
    "    if count == 1:\n",
    "        print(\"early stop\", \"-\"*20)\n",
    "        break\n",
    "print(\"학습 종료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52dab4d-96ec-4cf0-8ba6-7cfcb20ff306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_DataGeneration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
