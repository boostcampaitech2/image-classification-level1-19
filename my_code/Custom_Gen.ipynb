{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geographic-foster",
   "metadata": {
    "id": "geographic-foster",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Lesson 4 - Data Generation\n",
    "- 이번 실습자료에서는 파이토치 모델에 이미지를 입력값으로 주기위해 전처리를 하는 방법을 배웁니다.\n",
    "- 파이토치는 torch.utils.data에 있는 Dataset, DataLoader 클래스가 이 작업을 간편하게 해줍니다.\n",
    "## 0. Libraries & Configurations\n",
    "1- 시각화에 필요한 라이브러리와 데이터 경로를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "occasional-boxing",
   "metadata": {
    "id": "occasional-boxing"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import f1_score\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import resnet18, resnet50, resnet101\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "#define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "complex-israel",
   "metadata": {
    "id": "complex-israel"
   },
   "outputs": [],
   "source": [
    "### Configurations\n",
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/train_with_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "whole-computer",
   "metadata": {
    "id": "whole-computer"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_path)\n",
    "df_idx = {\"id\":0, \"gender\":1, \"age\":2, \"path\":3, \"name\":4, \"label\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "488b4014-2641-4526-9307-22f3de43247b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0     2745\n",
       "1     2050\n",
       "2      415\n",
       "3     3660\n",
       "4     4085\n",
       "5      545\n",
       "6      549\n",
       "7      410\n",
       "8       83\n",
       "9      732\n",
       "10     817\n",
       "11     109\n",
       "12     549\n",
       "13     410\n",
       "14      83\n",
       "15     732\n",
       "16     817\n",
       "17     109\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-neighborhood",
   "metadata": {
    "id": "minute-neighborhood"
   },
   "source": [
    "## 1. Image Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crude-frank",
   "metadata": {
    "id": "crude-frank"
   },
   "outputs": [],
   "source": [
    "def get_ext(img_dir, img_id):\n",
    "    filename = os.listdir(os.path.join(img_dir, img_id))[0]\n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "superb-profession",
   "metadata": {
    "id": "superb-profession"
   },
   "outputs": [],
   "source": [
    "def get_img_stats(img_ids):\n",
    "    img_info = dict(heights=[], widths=[], means=[], stds=[])\n",
    "    for path in tqdm.tqdm(img_ids):\n",
    "        img = np.array(Image.open(path))\n",
    "        h, w, _ = img.shape\n",
    "        img_info['heights'].append(h)\n",
    "        img_info['widths'].append(w)\n",
    "        img_info['means'].append(img.mean(axis=(0,1)))\n",
    "        img_info['stds'].append(img.std(axis=(0,1)))\n",
    "    return img_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9630245d-1c19-4a03-9490-fc9830f511fa",
   "metadata": {
    "id": "undefined-patrol",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18900/18900 [06:46<00:00, 46.54it/s]\n"
     ]
    }
   ],
   "source": [
    "img_info = get_img_stats(df.path.values)\n",
    "#print(f'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}')\n",
    "#print(f'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da16d75-7b84-4209-812c-795ddaf67a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB Mean: [0.56019358 0.52410121 0.501457  ]\n",
      "RGB Standard Deviation: [0.23318603 0.24300033 0.24567522]\n"
     ]
    }
   ],
   "source": [
    "print(f'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}')\n",
    "print(f'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-juvenile",
   "metadata": {
    "id": "democratic-juvenile",
    "tags": []
   },
   "source": [
    "## 2.1 Augmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "horizontal-strain",
   "metadata": {
    "id": "horizontal-strain"
   },
   "outputs": [],
   "source": [
    "mean, std = (0.5, 0.5, 0.5), (0.2, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "national-diameter",
   "metadata": {
    "id": "national-diameter"
   },
   "outputs": [],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "divine-transmission",
   "metadata": {
    "id": "divine-transmission"
   },
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    class_labels = []\n",
    "    image_images = []\n",
    "    \n",
    "    def __init__(self, data_df, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "        self.df = data_df\n",
    "\n",
    "        self.setup()\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        for index in tqdm.tqdm(range(self.__len__())):\n",
    "            df_series = self.df.iloc[index]\n",
    "            img_path = df_series[df_idx[\"path\"]]\n",
    "            if os.path.exists(img_path):\n",
    "                self.image_images.append(self.transform(image=np.array(Image.open(img_path)))['image'])\n",
    "                self.class_labels.append(df_series[df_idx[\"label\"]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        #image_path = self.image_paths[index]\n",
    "        image = self.image_images[index]\n",
    "        class_label = self.class_labels[index]\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        #image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image, class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "582e99c2-969b-4d3c-952b-e819be35db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7eaff3d-e76b-4b45-aab2-40225263ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69a03de2-be3b-45f2-9ab2-022ac06a0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = []\n",
    "for drop_idx in train_df.index:\n",
    "    if train_df.loc[drop_idx]['label'] == 0:\n",
    "        if np.random.randint(6) != 0:\n",
    "            drop_list.append(drop_idx)\n",
    "    elif train_df.loc[drop_idx]['label'] == 1:\n",
    "        if np.random.randint(5) != 0:\n",
    "            drop_list.append(drop_idx)\n",
    "    elif train_df.loc[drop_idx]['label'] == 3:\n",
    "        if np.random.randint(9) != 0:\n",
    "            drop_list.append(drop_idx)\n",
    "    elif train_df.loc[drop_idx]['label'] == 4:\n",
    "        if np.random.randint(10) != 0:\n",
    "            drop_list.append(drop_idx)\n",
    "    elif train_df.loc[drop_idx]['label'] in [9, 10, 12, 15, 16]:\n",
    "        if np.random.randint(2) != 0:\n",
    "            drop_list.append(drop_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "503c5f98-5dcd-47cb-91f3-d4a44a23f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train_df = train_df.drop(drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d2f851d-0c8e-4e1f-90c9-850b88ea4cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0     373\n",
       "1     312\n",
       "2     324\n",
       "3     315\n",
       "4     361\n",
       "5     437\n",
       "6     443\n",
       "7     316\n",
       "8      64\n",
       "9     304\n",
       "10    314\n",
       "11     87\n",
       "12    225\n",
       "13    319\n",
       "14     67\n",
       "15    289\n",
       "16    319\n",
       "17     91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_train_df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "secure-plasma",
   "metadata": {
    "id": "secure-plasma"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4960/4960 [02:18<00:00, 35.82it/s]\n",
      "100%|██████████| 3780/3780 [00:29<00:00, 130.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167.4860429763794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "start = time()\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "train_dataset = MaskBaseDataset(\n",
    "    data_df=drop_train_df, transform=transform[\"train\"]\n",
    ")\n",
    "\n",
    "val_dataset = MaskBaseDataset(\n",
    "    data_df=val_df, transform=transform[\"val\"]\n",
    ")\n",
    "\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1462c2e4-a685-4d6c-b076-a2621e01c648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3780"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-membrane",
   "metadata": {
    "id": "regulation-membrane"
   },
   "source": [
    "## 3. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "blessed-robert",
   "metadata": {
    "id": "blessed-robert"
   },
   "outputs": [],
   "source": [
    "# training dataloader은 데이터를 섞어주어야 합니다. (shuffle=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c4f390a-f6c2-4bd4-a93c-4258080252a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd0e901e-1fb7-4b3a-94a8-bc7a9c75b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b350aa32-c6db-44ea-9a91-15a575ab331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_finetune(classes):\n",
    "    #model = EfficientNet.from_pretrained('efficientnet-b2', num_classes=classes)\n",
    "    model = resnet18(pretrained=True)\n",
    "    # class 18개로 분리\n",
    "    model.fc = torch.nn.Linear(in_features=512, out_features=classes, bias=True)\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "    stdv = 1/math.sqrt(512)\n",
    "    model.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37a8c9f7-eab2-42c5-8318-96fb111ff383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3be6aee-9549-4e6c-a18c-61861e982daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_resnet = resnet_finetune(18).to(device)\n",
    "loss_fn = (torch.nn.CrossEntropyLoss())\n",
    "optimizer = torch.optim.Adam(my_resnet.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eeb0211b-2e8f-4825-ade8-04ba5d485a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_loader, \"test\": val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff09e0fc-cf5f-4a29-90bf-b8ecd43e6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "f1_best = 0\n",
    "count = 0\n",
    "epoch_f1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "287b2a6c-421c-4591-b8be-5055e1634035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 1.469, 평균 Accuracy : 0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.539, 평균 Accuracy : 0.830\n",
      "best :  0.709441317260629 , last :  0.709441317260629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.418, 평균 Accuracy : 0.878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss : 0.254, 평균 Accuracy : 0.942\n",
      "best :  0.9087666766289629 , last :  0.9087666766289629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.207, 평균 Accuracy : 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss : 0.124, 평균 Accuracy : 0.983\n",
      "best :  0.9699741145898846 , last :  0.9699741145898846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss : 0.096, 평균 Accuracy : 0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 test-데이터 셋에서 평균 Loss : 0.057, 평균 Accuracy : 0.999\n",
      "best :  0.9990483063083234 , last :  0.9990483063083234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss : 0.044, 평균 Accuracy : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-4의 test-데이터 셋에서 평균 Loss : 0.028, 평균 Accuracy : 1.000\n",
      "best :  1.0 , last :  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-5의 train-데이터 셋에서 평균 Loss : 0.024, 평균 Accuracy : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-5의 test-데이터 셋에서 평균 Loss : 0.015, 평균 Accuracy : 1.000\n",
      "best :  1.0 , last :  1.0\n",
      "early stop --------------------\n",
      "학습 종료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        if phase == \"train\":\n",
    "            my_resnet.train()\n",
    "        elif phase == \"test\":\n",
    "            my_resnet.eval()\n",
    "        \n",
    "        # 배치 단위로 data load하여서 작업 -> 이때 transpose 및 여러 함수가 적용된다.\n",
    "        epoch_f1 = 0\n",
    "        n_iters=0\n",
    "        for ind, (images, labels) in enumerate(tqdm.tqdm(dataloaders[phase], leave=False)):\n",
    "            images = torch.stack(list(images), dim=0).to(device)\n",
    "            labels = torch.tensor(list(labels)).to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # parameter gradient를 업데이트 전 초기화함\n",
    "\n",
    "            with torch.set_grad_enabled(\n",
    "                phase == \"train\"\n",
    "            ):  \n",
    "                logits = my_resnet(images)\n",
    "                _, preds = torch.max(\n",
    "                    logits, 1\n",
    "                )\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()  # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient\n",
    "                    optimizer.step()  # 계산된 gradient를 가지고 모델 업데이트\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_acc += torch.sum(\n",
    "                preds == labels.data\n",
    "            )\n",
    "            epoch_f1 += f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "            n_iters += 1\n",
    "\n",
    "        # 한 epoch이 모두 종료되었을 때,\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}\")\n",
    "        if phase == \"test\":\n",
    "            epoch_f1 /= n_iters\n",
    "            if epoch_f1 > f1_best:\n",
    "                f1_best = epoch_f1\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "            print(\"best : \", f1_best, \", last : \", epoch_f1)\n",
    "    if count == 1:\n",
    "        print(\"early stop\", \"-\"*20)\n",
    "        break\n",
    "print(\"학습 종료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d4f75d3-d4b4-4343-a37e-7043c0d19658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best :  1.0 , last :  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"best : \", f1_best, \", last : \", epoch_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48ca732b-c694-4788-b1bb-556dc04e0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = '/opt/ml/input/data/eval'\n",
    "test_dir = \"/opt/ml/input/data/eval\"\n",
    "submission = pd.read_csv(os.path.join(test_dir, \"info.csv\"))\n",
    "image_dir = os.path.join(test_dir, \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52a08649-9b79-46b1-947a-ed89d6856cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d83c7e9-42de-41af-8409-e6a38ffec1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7dd2c743-8913-4e7a-baa9-640a9259b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "99fdc55e-0ecf-4814-9b73-b2030b250d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "251b0ef2-b515-4e1f-9e56-a315cbbb4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TestDataset(image_paths, test_transform)\n",
    "loader = data.DataLoader(dataset, shuffle=False)\n",
    "\n",
    "my_resnet.eval()\n",
    "all_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe2f1b5f-c105-42b7-947d-afc7847c9ea8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'You have to pass data to augmentations as named arguments, for example: aug(image=image)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-4e5a2b4c7187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-c54cf6254547>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to pass data to augmentations as named arguments, for example: aug(image=image)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplied_in_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'You have to pass data to augmentations as named arguments, for example: aug(image=image)'"
     ]
    }
   ],
   "source": [
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = my_resnet(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "print(\"test inference is done!\")\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bd60a8b7-5ad8-4f4f-b591-1043eab98213",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('/opt/ml/input/data/eval/submission_resnet18_10_0001_256_undersample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653bc78-8a1d-4b6c-b4cd-597a3cb6876e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_DataGeneration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
